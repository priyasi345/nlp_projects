# -*- coding: utf-8 -*-
"""Text Summarization_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/103sxP2VXjlOkrVNSP28I_23a9NZR7qGy
"""

!pip install -U spacy

!python -m spacy download en_core_web_sm

text="""
In computer science, artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of "intelligent agents": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.[1] Colloquially, the term "artificial intelligence" is often used to describe machines (or computers) that mimic "cognitive" functions that humans associate with the human mind, such as "learning" and "problem solving """

import spacy
from spacy.lang.en.stop_words import  STOP_WORDS
from string import punctuation

stopwords=list(STOP_WORDS)
stopwords

nlp=spacy.load('en_core_web_sm')

doc=nlp(text)

tokens=[token.text for token in doc]
print(tokens)

punctuation=punctuation + '\n'
punctuation

word_frequencies={}
for word in doc:
  if word.text.lower() not in stopwords:
    if word.text.lower() not in punctuation:
      if word.text not in word_frequencies.keys():
        word_frequencies[word.text]=1
      else:
        word_frequencies[word.text]+=1

print(word_frequencies)

max_frequency=max(word_frequencies.values())

max_frequency

for word in word_frequencies.keys():
  word_frequencies[word]=word_frequencies[word]/max_frequency

sentence_tokens=[sent for sent in doc.sents]
sentence_tokens

sentence_scores={}
for sent in sentence_tokens:
  for word in sent:
    if word.text.lower() in word_frequencies.keys():
      if sent not in sentence_scores.keys():
        sentence_scores[sent] = word_frequencies[word.text.lower()]
      else:
         sentence_scores[sent] += word_frequencies[word.text.lower()]

sentence_scores

from heapq import nlargest

select_length=int(len(sentence_tokens)*0.9)
select_length

summary=nlargest(select_length,sentence_scores,key=sentence_scores.get)

summary

final_summary=[word.text for word in summary]

summary=' '.join(final_summary)

summary